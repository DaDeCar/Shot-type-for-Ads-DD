{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tensorflow_docs.vis import embed\n",
    "from tensorflow import keras\n",
    "#from imutils import paths\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import imageio\n",
    "import cv2\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd. read_csv('/workspaces/final-project-shot-type/data/CSV/dataset_df.csv', dtype={\"shot\": str})\n",
    "\n",
    "data_df.drop(columns=['scale_val'], inplace=True)\n",
    "data_df.drop(columns = ['move_val'], inplace=True)\n",
    "\n",
    "train_path = '/workspaces/final-project-shot-type/data/shot-type-dataset/trailer_v3/train/'\n",
    "data_df['video_name'] = train_path + 'shot_' + data_df['movie'] + '_' + data_df['shot'] + '.mp4'\n",
    "train_df = data_df[data_df['dataset']=='train']\n",
    "train_df = train_df[['video_name', 'move_label','scale_label']]\n",
    "train_df = train_df.rename(columns ={'scale_label': 'tag'})\n",
    "\n",
    "\n",
    "test_path = '/workspaces/final-project-shot-type/data/shot-type-dataset/trailer_v3/test/'\n",
    "data_df['video_name'] = test_path + 'shot_' + data_df['movie'] + '_' + data_df['shot'] + '.mp4'\n",
    "test_df = data_df[data_df['dataset']=='test']\n",
    "test_df = test_df[['video_name', 'move_label','scale_label']]\n",
    "test_df = test_df.rename(columns ={'scale_label': 'tag'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_name</th>\n",
       "      <th>move_label</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/workspaces/final-project-shot-type/data/shot-...</td>\n",
       "      <td>Static</td>\n",
       "      <td>CS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/workspaces/final-project-shot-type/data/shot-...</td>\n",
       "      <td>Static</td>\n",
       "      <td>CS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/workspaces/final-project-shot-type/data/shot-...</td>\n",
       "      <td>Static</td>\n",
       "      <td>ECS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/workspaces/final-project-shot-type/data/shot-...</td>\n",
       "      <td>Static</td>\n",
       "      <td>MS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/workspaces/final-project-shot-type/data/shot-...</td>\n",
       "      <td>Static</td>\n",
       "      <td>CS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20851</th>\n",
       "      <td>/workspaces/final-project-shot-type/data/shot-...</td>\n",
       "      <td>Push</td>\n",
       "      <td>FS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20852</th>\n",
       "      <td>/workspaces/final-project-shot-type/data/shot-...</td>\n",
       "      <td>Motion</td>\n",
       "      <td>LS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20853</th>\n",
       "      <td>/workspaces/final-project-shot-type/data/shot-...</td>\n",
       "      <td>Push</td>\n",
       "      <td>LS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20854</th>\n",
       "      <td>/workspaces/final-project-shot-type/data/shot-...</td>\n",
       "      <td>Motion</td>\n",
       "      <td>FS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20855</th>\n",
       "      <td>/workspaces/final-project-shot-type/data/shot-...</td>\n",
       "      <td>Static</td>\n",
       "      <td>LS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20856 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              video_name move_label  tag\n",
       "0      /workspaces/final-project-shot-type/data/shot-...     Static   CS\n",
       "1      /workspaces/final-project-shot-type/data/shot-...     Static   CS\n",
       "2      /workspaces/final-project-shot-type/data/shot-...     Static  ECS\n",
       "3      /workspaces/final-project-shot-type/data/shot-...     Static   MS\n",
       "4      /workspaces/final-project-shot-type/data/shot-...     Static   CS\n",
       "...                                                  ...        ...  ...\n",
       "20851  /workspaces/final-project-shot-type/data/shot-...       Push   FS\n",
       "20852  /workspaces/final-project-shot-type/data/shot-...     Motion   LS\n",
       "20853  /workspaces/final-project-shot-type/data/shot-...       Push   LS\n",
       "20854  /workspaces/final-project-shot-type/data/shot-...     Motion   FS\n",
       "20855  /workspaces/final-project-shot-type/data/shot-...     Static   LS\n",
       "\n",
       "[20856 rows x 3 columns]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_name</th>\n",
       "      <th>move_label</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25466</th>\n",
       "      <td>/workspaces/final-project-shot-type/data/shot-...</td>\n",
       "      <td>Static</td>\n",
       "      <td>LS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25467</th>\n",
       "      <td>/workspaces/final-project-shot-type/data/shot-...</td>\n",
       "      <td>Static</td>\n",
       "      <td>MS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25468</th>\n",
       "      <td>/workspaces/final-project-shot-type/data/shot-...</td>\n",
       "      <td>Static</td>\n",
       "      <td>MS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25469</th>\n",
       "      <td>/workspaces/final-project-shot-type/data/shot-...</td>\n",
       "      <td>Static</td>\n",
       "      <td>LS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25470</th>\n",
       "      <td>/workspaces/final-project-shot-type/data/shot-...</td>\n",
       "      <td>Motion</td>\n",
       "      <td>FS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33648</th>\n",
       "      <td>/workspaces/final-project-shot-type/data/shot-...</td>\n",
       "      <td>Static</td>\n",
       "      <td>FS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33649</th>\n",
       "      <td>/workspaces/final-project-shot-type/data/shot-...</td>\n",
       "      <td>Motion</td>\n",
       "      <td>MS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33650</th>\n",
       "      <td>/workspaces/final-project-shot-type/data/shot-...</td>\n",
       "      <td>Motion</td>\n",
       "      <td>FS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33651</th>\n",
       "      <td>/workspaces/final-project-shot-type/data/shot-...</td>\n",
       "      <td>Static</td>\n",
       "      <td>LS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33652</th>\n",
       "      <td>/workspaces/final-project-shot-type/data/shot-...</td>\n",
       "      <td>Static</td>\n",
       "      <td>ECS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8187 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              video_name move_label  tag\n",
       "25466  /workspaces/final-project-shot-type/data/shot-...     Static   LS\n",
       "25467  /workspaces/final-project-shot-type/data/shot-...     Static   MS\n",
       "25468  /workspaces/final-project-shot-type/data/shot-...     Static   MS\n",
       "25469  /workspaces/final-project-shot-type/data/shot-...     Static   LS\n",
       "25470  /workspaces/final-project-shot-type/data/shot-...     Motion   FS\n",
       "...                                                  ...        ...  ...\n",
       "33648  /workspaces/final-project-shot-type/data/shot-...     Static   FS\n",
       "33649  /workspaces/final-project-shot-type/data/shot-...     Motion   MS\n",
       "33650  /workspaces/final-project-shot-type/data/shot-...     Motion   FS\n",
       "33651  /workspaces/final-project-shot-type/data/shot-...     Static   LS\n",
       "33652  /workspaces/final-project-shot-type/data/shot-...     Static  ECS\n",
       "\n",
       "[8187 rows x 3 columns]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train.to_csv('/workspaces/final-project-shot-type/data/CSV/dataset_train_v1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_test.to_csv('/workspaces/final-project-shot-type/data/CSV/dataset_test_v1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total videos for training: 10000\n",
      "Total videos for testing: 3600\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_name</th>\n",
       "      <th>move_label</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3156</th>\n",
       "      <td>/workspaces/final-project-shot-type/data/shot-...</td>\n",
       "      <td>Static</td>\n",
       "      <td>ECS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5691</th>\n",
       "      <td>/workspaces/final-project-shot-type/data/shot-...</td>\n",
       "      <td>Motion</td>\n",
       "      <td>LS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9706</th>\n",
       "      <td>/workspaces/final-project-shot-type/data/shot-...</td>\n",
       "      <td>Static</td>\n",
       "      <td>FS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4834</th>\n",
       "      <td>/workspaces/final-project-shot-type/data/shot-...</td>\n",
       "      <td>Static</td>\n",
       "      <td>MS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923</th>\n",
       "      <td>/workspaces/final-project-shot-type/data/shot-...</td>\n",
       "      <td>Motion</td>\n",
       "      <td>CS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9914</th>\n",
       "      <td>/workspaces/final-project-shot-type/data/shot-...</td>\n",
       "      <td>Static</td>\n",
       "      <td>MS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9338</th>\n",
       "      <td>/workspaces/final-project-shot-type/data/shot-...</td>\n",
       "      <td>Static</td>\n",
       "      <td>CS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8806</th>\n",
       "      <td>/workspaces/final-project-shot-type/data/shot-...</td>\n",
       "      <td>Static</td>\n",
       "      <td>LS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7390</th>\n",
       "      <td>/workspaces/final-project-shot-type/data/shot-...</td>\n",
       "      <td>Static</td>\n",
       "      <td>LS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5902</th>\n",
       "      <td>/workspaces/final-project-shot-type/data/shot-...</td>\n",
       "      <td>Static</td>\n",
       "      <td>MS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             video_name move_label  tag\n",
       "3156  /workspaces/final-project-shot-type/data/shot-...     Static  ECS\n",
       "5691  /workspaces/final-project-shot-type/data/shot-...     Motion   LS\n",
       "9706  /workspaces/final-project-shot-type/data/shot-...     Static   FS\n",
       "4834  /workspaces/final-project-shot-type/data/shot-...     Static   MS\n",
       "923   /workspaces/final-project-shot-type/data/shot-...     Motion   CS\n",
       "9914  /workspaces/final-project-shot-type/data/shot-...     Static   MS\n",
       "9338  /workspaces/final-project-shot-type/data/shot-...     Static   CS\n",
       "8806  /workspaces/final-project-shot-type/data/shot-...     Static   LS\n",
       "7390  /workspaces/final-project-shot-type/data/shot-...     Static   LS\n",
       "5902  /workspaces/final-project-shot-type/data/shot-...     Static   MS"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df= train_df[0:10000]\n",
    "\n",
    "# train_df = pd.read_csv('/workspaces/final-project-shot-type/data/CSV/dataset_train_v1.csv')\n",
    "test_df = test_df[0:3600]\n",
    "\n",
    "print(f\"Total videos for training: {len(train_df)}\")\n",
    "print(f\"Total videos for testing: {len(test_df)}\")\n",
    "\n",
    "train_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CS     2230\n",
       "MS     2204\n",
       "ECS    2022\n",
       "FS     1978\n",
       "LS     1566\n",
       "Name: tag, dtype: int64"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['tag'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 50\n",
    "\n",
    "MAX_SEQ_LENGTH = 10\n",
    "NUM_FEATURES = 2048"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading frames and working on frame's size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_center_square(frame):\n",
    "    y, x = frame.shape[0:2]\n",
    "    min_dim = min(y, x)\n",
    "    start_x = (x // 2) - (min_dim // 2)\n",
    "    start_y = (y // 2) - (min_dim // 2)\n",
    "    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]\n",
    "\n",
    "\n",
    "def load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = []\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = crop_center_square(frame)\n",
    "            frame = cv2.resize(frame, resize)\n",
    "            frame = frame[:, :, [2, 1, 0]]\n",
    "            frames.append(frame)\n",
    "\n",
    "            if len(frames) == max_frames:\n",
    "                break\n",
    "    finally:\n",
    "        cap.release()\n",
    "    return np.array(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"feature_extractor\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_34 (InputLayer)       [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " tf.math.truediv_4 (TFOpLamb  (None, 224, 224, 3)      0         \n",
      " da)                                                             \n",
      "                                                                 \n",
      " tf.math.subtract_4 (TFOpLam  (None, 224, 224, 3)      0         \n",
      " bda)                                                            \n",
      "                                                                 \n",
      " inception_v3 (Functional)   (None, 2048)              21802784  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21,802,784\n",
      "Trainable params: 21,768,352\n",
      "Non-trainable params: 34,432\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def build_feature_extractor():\n",
    "    feature_extractor = keras.applications.InceptionV3(\n",
    "        weights=\"imagenet\",\n",
    "        include_top=False,\n",
    "        pooling=\"avg\",\n",
    "        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "    )\n",
    "    preprocess_input = keras.applications.inception_v3.preprocess_input\n",
    "\n",
    "    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
    "    preprocessed = preprocess_input(inputs)\n",
    "\n",
    "    outputs = feature_extractor(preprocessed)\n",
    "    return keras.Model(inputs, outputs, name=\"feature_extractor\")\n",
    "\n",
    "\n",
    "feature_extractor = build_feature_extractor()\n",
    "print(feature_extractor.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working on labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CS', 'ECS', 'FS', 'LS', 'MS']\n"
     ]
    }
   ],
   "source": [
    "label_processor = keras.layers.StringLookup(\n",
    "    num_oov_indices=0, vocabulary=np.unique(train_df[\"tag\"])\n",
    ")\n",
    "print(label_processor.get_vocabulary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame features in train set: (10000, 10, 2048)\n",
      "Frame masks in train set: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "def prepare_all_videos(df, root_dir):\n",
    "    num_samples = len(df)\n",
    "    video_paths = df[\"video_name\"].values.tolist()\n",
    "    labels = df[\"tag\"].values\n",
    "    labels = label_processor(labels[..., None]).numpy()\n",
    "\n",
    "    # `frame_masks` and `frame_features` are what we will feed to our sequence model.\n",
    "    # `frame_masks` will contain a bunch of booleans denoting if a timestep is\n",
    "    # masked with padding or not.\n",
    "    frame_masks = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH), dtype=\"bool\")\n",
    "    frame_features = np.zeros(\n",
    "        shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
    "    )\n",
    "\n",
    "    # For each video.\n",
    "    for idx, path in enumerate(video_paths):\n",
    "        # Gather all its frames and add a batch dimension.\n",
    "        frames = load_video(os.path.join(root_dir, path))\n",
    "        frames = frames[None, ...]\n",
    "\n",
    "        # Initialize placeholders to store the masks and features of the current video.\n",
    "        temp_frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "        temp_frame_features = np.zeros(\n",
    "            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
    "        )\n",
    "\n",
    "        # Extract features from the frames of the current video.\n",
    "        for i, batch in enumerate(frames):\n",
    "            video_length = batch.shape[0]\n",
    "            length = min(MAX_SEQ_LENGTH, video_length)\n",
    "            for j in range(length):\n",
    "                temp_frame_features[i, j, :] = feature_extractor.predict(\n",
    "                    batch[None, j, :]\n",
    "                )\n",
    "            temp_frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n",
    "\n",
    "        frame_features[idx,] = temp_frame_features.squeeze()\n",
    "        frame_masks[idx,] = temp_frame_mask.squeeze()\n",
    "\n",
    "    return (frame_features, frame_masks), labels\n",
    "\n",
    "\n",
    "train_data, train_labels = prepare_all_videos(train_df, \"train\")\n",
    "test_data, test_labels = prepare_all_videos(test_df, \"test\")\n",
    "\n",
    "print(f\"Frame features in train set: {train_data[0].shape}\")\n",
    "print(f\"Frame masks in train set: {train_data[1].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_embedding_0_10k = np.save('/workspaces/final-project-shot-type/embeddings/train_data_embedding_0_10k.npy', train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_embedding_1_10k = np.save('/workspaces/final-project-shot-type/embeddings/train_data_embedding_1_10k.npy', train_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_embedding_10k = np.save('/workspaces/final-project-shot-type/embeddings/train_labels_embedding_10k.npy', train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_embedding_0_10k = np.save('/workspaces/final-project-shot-type/embeddings/test_data_embedding_0_10k.npy', test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_embedding_1_10k = np.save('/workspaces/final-project-shot-type/embeddings/test_data_embedding_1_10k.npy', test_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels_embedding_10k = np.save('/workspaces/final-project-shot-type/embeddings/test_labels_embedding_10k.npy', test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequence model (RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "218/219 [============================>.] - ETA: 0s - loss: 1.5007 - accuracy: 0.3019\n",
      "Epoch 1: val_loss improved from inf to 1.33294, saving model to /workspaces/final-project-shot-type/experiments/inceptionv3_GRU/exp_005/model.weights_exp_005.h5\n",
      "219/219 [==============================] - 23s 47ms/step - loss: 1.5002 - accuracy: 0.3024 - val_loss: 1.3329 - val_accuracy: 0.4460 - lr: 1.0000e-04\n",
      "Epoch 2/50\n",
      "218/219 [============================>.] - ETA: 0s - loss: 1.3192 - accuracy: 0.4167\n",
      "Epoch 2: val_loss improved from 1.33294 to 1.20140, saving model to /workspaces/final-project-shot-type/experiments/inceptionv3_GRU/exp_005/model.weights_exp_005.h5\n",
      "219/219 [==============================] - 5s 24ms/step - loss: 1.3192 - accuracy: 0.4167 - val_loss: 1.2014 - val_accuracy: 0.5110 - lr: 1.0000e-04\n",
      "Epoch 3/50\n",
      "219/219 [==============================] - ETA: 0s - loss: 1.2210 - accuracy: 0.4667\n",
      "Epoch 3: val_loss improved from 1.20140 to 1.12294, saving model to /workspaces/final-project-shot-type/experiments/inceptionv3_GRU/exp_005/model.weights_exp_005.h5\n",
      "219/219 [==============================] - 5s 22ms/step - loss: 1.2210 - accuracy: 0.4667 - val_loss: 1.1229 - val_accuracy: 0.5443 - lr: 1.0000e-04\n",
      "Epoch 4/50\n",
      "217/219 [============================>.] - ETA: 0s - loss: 1.1468 - accuracy: 0.5105\n",
      "Epoch 4: val_loss improved from 1.12294 to 1.05963, saving model to /workspaces/final-project-shot-type/experiments/inceptionv3_GRU/exp_005/model.weights_exp_005.h5\n",
      "219/219 [==============================] - 5s 23ms/step - loss: 1.1464 - accuracy: 0.5101 - val_loss: 1.0596 - val_accuracy: 0.5487 - lr: 1.0000e-04\n",
      "Epoch 5/50\n",
      "217/219 [============================>.] - ETA: 0s - loss: 1.0769 - accuracy: 0.5422\n",
      "Epoch 5: val_loss improved from 1.05963 to 1.00045, saving model to /workspaces/final-project-shot-type/experiments/inceptionv3_GRU/exp_005/model.weights_exp_005.h5\n",
      "219/219 [==============================] - 5s 23ms/step - loss: 1.0764 - accuracy: 0.5426 - val_loss: 1.0005 - val_accuracy: 0.5893 - lr: 1.0000e-04\n",
      "Epoch 6/50\n",
      "217/219 [============================>.] - ETA: 0s - loss: 1.0172 - accuracy: 0.5680\n",
      "Epoch 6: val_loss improved from 1.00045 to 0.94651, saving model to /workspaces/final-project-shot-type/experiments/inceptionv3_GRU/exp_005/model.weights_exp_005.h5\n",
      "219/219 [==============================] - 5s 23ms/step - loss: 1.0177 - accuracy: 0.5671 - val_loss: 0.9465 - val_accuracy: 0.6113 - lr: 1.0000e-04\n",
      "Epoch 7/50\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.9710 - accuracy: 0.5991\n",
      "Epoch 7: val_loss improved from 0.94651 to 0.89975, saving model to /workspaces/final-project-shot-type/experiments/inceptionv3_GRU/exp_005/model.weights_exp_005.h5\n",
      "219/219 [==============================] - 5s 23ms/step - loss: 0.9710 - accuracy: 0.5991 - val_loss: 0.8997 - val_accuracy: 0.6380 - lr: 1.0000e-04\n",
      "Epoch 8/50\n",
      "217/219 [============================>.] - ETA: 0s - loss: 0.9277 - accuracy: 0.6208\n",
      "Epoch 8: val_loss improved from 0.89975 to 0.87285, saving model to /workspaces/final-project-shot-type/experiments/inceptionv3_GRU/exp_005/model.weights_exp_005.h5\n",
      "219/219 [==============================] - 5s 23ms/step - loss: 0.9270 - accuracy: 0.6211 - val_loss: 0.8729 - val_accuracy: 0.6887 - lr: 1.0000e-04\n",
      "Epoch 9/50\n",
      "217/219 [============================>.] - ETA: 0s - loss: 0.8910 - accuracy: 0.6519\n",
      "Epoch 9: val_loss improved from 0.87285 to 0.84146, saving model to /workspaces/final-project-shot-type/experiments/inceptionv3_GRU/exp_005/model.weights_exp_005.h5\n",
      "219/219 [==============================] - 5s 23ms/step - loss: 0.8909 - accuracy: 0.6519 - val_loss: 0.8415 - val_accuracy: 0.6963 - lr: 1.0000e-04\n",
      "Epoch 10/50\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.8549 - accuracy: 0.6776\n",
      "Epoch 10: val_loss improved from 0.84146 to 0.82224, saving model to /workspaces/final-project-shot-type/experiments/inceptionv3_GRU/exp_005/model.weights_exp_005.h5\n",
      "219/219 [==============================] - 6s 28ms/step - loss: 0.8549 - accuracy: 0.6776 - val_loss: 0.8222 - val_accuracy: 0.6933 - lr: 1.0000e-04\n",
      "Epoch 11/50\n",
      "217/219 [============================>.] - ETA: 0s - loss: 0.8253 - accuracy: 0.6957\n",
      "Epoch 11: val_loss improved from 0.82224 to 0.80989, saving model to /workspaces/final-project-shot-type/experiments/inceptionv3_GRU/exp_005/model.weights_exp_005.h5\n",
      "219/219 [==============================] - 6s 27ms/step - loss: 0.8238 - accuracy: 0.6961 - val_loss: 0.8099 - val_accuracy: 0.6857 - lr: 1.0000e-04\n",
      "Epoch 12/50\n",
      "218/219 [============================>.] - ETA: 0s - loss: 0.7872 - accuracy: 0.6984\n",
      "Epoch 12: val_loss improved from 0.80989 to 0.77771, saving model to /workspaces/final-project-shot-type/experiments/inceptionv3_GRU/exp_005/model.weights_exp_005.h5\n",
      "219/219 [==============================] - 5s 25ms/step - loss: 0.7870 - accuracy: 0.6986 - val_loss: 0.7777 - val_accuracy: 0.7083 - lr: 1.0000e-04\n",
      "Epoch 13/50\n",
      "217/219 [============================>.] - ETA: 0s - loss: 0.7620 - accuracy: 0.7124\n",
      "Epoch 13: val_loss improved from 0.77771 to 0.75787, saving model to /workspaces/final-project-shot-type/experiments/inceptionv3_GRU/exp_005/model.weights_exp_005.h5\n",
      "219/219 [==============================] - 6s 27ms/step - loss: 0.7611 - accuracy: 0.7131 - val_loss: 0.7579 - val_accuracy: 0.7077 - lr: 1.0000e-04\n",
      "Epoch 14/50\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.7487 - accuracy: 0.7193\n",
      "Epoch 14: val_loss did not improve from 0.75787\n",
      "219/219 [==============================] - 6s 27ms/step - loss: 0.7487 - accuracy: 0.7193 - val_loss: 0.7733 - val_accuracy: 0.6910 - lr: 1.0000e-04\n",
      "Epoch 15/50\n",
      "218/219 [============================>.] - ETA: 0s - loss: 0.7190 - accuracy: 0.7268\n",
      "Epoch 15: val_loss improved from 0.75787 to 0.73941, saving model to /workspaces/final-project-shot-type/experiments/inceptionv3_GRU/exp_005/model.weights_exp_005.h5\n",
      "219/219 [==============================] - 6s 27ms/step - loss: 0.7191 - accuracy: 0.7263 - val_loss: 0.7394 - val_accuracy: 0.7120 - lr: 1.0000e-04\n",
      "Epoch 16/50\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.7001 - accuracy: 0.7416\n",
      "Epoch 16: val_loss improved from 0.73941 to 0.72261, saving model to /workspaces/final-project-shot-type/experiments/inceptionv3_GRU/exp_005/model.weights_exp_005.h5\n",
      "219/219 [==============================] - 5s 25ms/step - loss: 0.7001 - accuracy: 0.7416 - val_loss: 0.7226 - val_accuracy: 0.7260 - lr: 1.0000e-04\n",
      "Epoch 17/50\n",
      "218/219 [============================>.] - ETA: 0s - loss: 0.6715 - accuracy: 0.7466\n",
      "Epoch 17: val_loss improved from 0.72261 to 0.69030, saving model to /workspaces/final-project-shot-type/experiments/inceptionv3_GRU/exp_005/model.weights_exp_005.h5\n",
      "219/219 [==============================] - 6s 27ms/step - loss: 0.6711 - accuracy: 0.7466 - val_loss: 0.6903 - val_accuracy: 0.7560 - lr: 1.0000e-04\n",
      "Epoch 18/50\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.6498 - accuracy: 0.7671\n",
      "Epoch 18: val_loss improved from 0.69030 to 0.68072, saving model to /workspaces/final-project-shot-type/experiments/inceptionv3_GRU/exp_005/model.weights_exp_005.h5\n",
      "219/219 [==============================] - 6s 28ms/step - loss: 0.6498 - accuracy: 0.7671 - val_loss: 0.6807 - val_accuracy: 0.7620 - lr: 1.0000e-04\n",
      "Epoch 19/50\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.6349 - accuracy: 0.7720\n",
      "Epoch 19: val_loss improved from 0.68072 to 0.67366, saving model to /workspaces/final-project-shot-type/experiments/inceptionv3_GRU/exp_005/model.weights_exp_005.h5\n",
      "219/219 [==============================] - 6s 27ms/step - loss: 0.6349 - accuracy: 0.7720 - val_loss: 0.6737 - val_accuracy: 0.7763 - lr: 1.0000e-04\n",
      "Epoch 20/50\n",
      "217/219 [============================>.] - ETA: 0s - loss: 0.6042 - accuracy: 0.7828\n",
      "Epoch 20: val_loss improved from 0.67366 to 0.66165, saving model to /workspaces/final-project-shot-type/experiments/inceptionv3_GRU/exp_005/model.weights_exp_005.h5\n",
      "219/219 [==============================] - 6s 26ms/step - loss: 0.6045 - accuracy: 0.7829 - val_loss: 0.6616 - val_accuracy: 0.7733 - lr: 1.0000e-04\n",
      "Epoch 21/50\n",
      "217/219 [============================>.] - ETA: 0s - loss: 0.5900 - accuracy: 0.7913\n",
      "Epoch 21: val_loss improved from 0.66165 to 0.64332, saving model to /workspaces/final-project-shot-type/experiments/inceptionv3_GRU/exp_005/model.weights_exp_005.h5\n",
      "219/219 [==============================] - 6s 25ms/step - loss: 0.5907 - accuracy: 0.7909 - val_loss: 0.6433 - val_accuracy: 0.7883 - lr: 1.0000e-04\n",
      "Epoch 22/50\n",
      "218/219 [============================>.] - ETA: 0s - loss: 0.5695 - accuracy: 0.7959\n",
      "Epoch 22: val_loss did not improve from 0.64332\n",
      "219/219 [==============================] - 6s 28ms/step - loss: 0.5699 - accuracy: 0.7956 - val_loss: 0.6536 - val_accuracy: 0.7807 - lr: 1.0000e-04\n",
      "Epoch 23/50\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.5542 - accuracy: 0.8103\n",
      "Epoch 23: val_loss improved from 0.64332 to 0.64265, saving model to /workspaces/final-project-shot-type/experiments/inceptionv3_GRU/exp_005/model.weights_exp_005.h5\n",
      "219/219 [==============================] - 6s 27ms/step - loss: 0.5542 - accuracy: 0.8103 - val_loss: 0.6426 - val_accuracy: 0.7910 - lr: 1.0000e-04\n",
      "Epoch 24/50\n",
      "218/219 [============================>.] - ETA: 0s - loss: 0.5412 - accuracy: 0.8068\n",
      "Epoch 24: val_loss improved from 0.64265 to 0.64037, saving model to /workspaces/final-project-shot-type/experiments/inceptionv3_GRU/exp_005/model.weights_exp_005.h5\n",
      "219/219 [==============================] - 6s 25ms/step - loss: 0.5406 - accuracy: 0.8070 - val_loss: 0.6404 - val_accuracy: 0.7870 - lr: 1.0000e-04\n",
      "Epoch 25/50\n",
      "218/219 [============================>.] - ETA: 0s - loss: 0.5231 - accuracy: 0.8217\n",
      "Epoch 25: val_loss improved from 0.64037 to 0.63248, saving model to /workspaces/final-project-shot-type/experiments/inceptionv3_GRU/exp_005/model.weights_exp_005.h5\n",
      "219/219 [==============================] - 6s 28ms/step - loss: 0.5228 - accuracy: 0.8219 - val_loss: 0.6325 - val_accuracy: 0.7860 - lr: 1.0000e-04\n",
      "Epoch 26/50\n",
      "218/219 [============================>.] - ETA: 0s - loss: 0.5073 - accuracy: 0.8290\n",
      "Epoch 26: val_loss improved from 0.63248 to 0.61733, saving model to /workspaces/final-project-shot-type/experiments/inceptionv3_GRU/exp_005/model.weights_exp_005.h5\n",
      "219/219 [==============================] - 6s 26ms/step - loss: 0.5073 - accuracy: 0.8287 - val_loss: 0.6173 - val_accuracy: 0.7957 - lr: 1.0000e-04\n",
      "Epoch 27/50\n",
      "217/219 [============================>.] - ETA: 0s - loss: 0.4862 - accuracy: 0.8355\n",
      "Epoch 27: val_loss did not improve from 0.61733\n",
      "219/219 [==============================] - 6s 27ms/step - loss: 0.4863 - accuracy: 0.8359 - val_loss: 0.6251 - val_accuracy: 0.7973 - lr: 1.0000e-04\n",
      "Epoch 28/50\n",
      "218/219 [============================>.] - ETA: 0s - loss: 0.4751 - accuracy: 0.8419\n",
      "Epoch 28: val_loss did not improve from 0.61733\n",
      "219/219 [==============================] - 6s 27ms/step - loss: 0.4746 - accuracy: 0.8423 - val_loss: 0.6214 - val_accuracy: 0.7927 - lr: 1.0000e-04\n",
      "Epoch 29/50\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.4619 - accuracy: 0.8427\n",
      "Epoch 29: val_loss improved from 0.61733 to 0.61499, saving model to /workspaces/final-project-shot-type/experiments/inceptionv3_GRU/exp_005/model.weights_exp_005.h5\n",
      "219/219 [==============================] - 6s 26ms/step - loss: 0.4619 - accuracy: 0.8427 - val_loss: 0.6150 - val_accuracy: 0.7930 - lr: 3.0000e-05\n",
      "Epoch 30/50\n",
      "217/219 [============================>.] - ETA: 0s - loss: 0.4427 - accuracy: 0.8561\n",
      "Epoch 30: val_loss improved from 0.61499 to 0.60854, saving model to /workspaces/final-project-shot-type/experiments/inceptionv3_GRU/exp_005/model.weights_exp_005.h5\n",
      "219/219 [==============================] - 6s 28ms/step - loss: 0.4427 - accuracy: 0.8563 - val_loss: 0.6085 - val_accuracy: 0.7927 - lr: 3.0000e-05\n",
      "Epoch 31/50\n",
      "217/219 [============================>.] - ETA: 0s - loss: 0.4478 - accuracy: 0.8576\n",
      "Epoch 31: val_loss did not improve from 0.60854\n",
      "219/219 [==============================] - 6s 28ms/step - loss: 0.4484 - accuracy: 0.8576 - val_loss: 0.6164 - val_accuracy: 0.7917 - lr: 3.0000e-05\n",
      "Epoch 32/50\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.4432 - accuracy: 0.8543\n",
      "Epoch 32: val_loss did not improve from 0.60854\n",
      "219/219 [==============================] - 6s 27ms/step - loss: 0.4432 - accuracy: 0.8543 - val_loss: 0.6170 - val_accuracy: 0.7947 - lr: 3.0000e-05\n",
      "Epoch 33/50\n",
      "218/219 [============================>.] - ETA: 0s - loss: 0.4371 - accuracy: 0.8585\n",
      "Epoch 33: val_loss did not improve from 0.60854\n",
      "219/219 [==============================] - 6s 27ms/step - loss: 0.4370 - accuracy: 0.8584 - val_loss: 0.6144 - val_accuracy: 0.7987 - lr: 9.0000e-06\n",
      "Epoch 34/50\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.4290 - accuracy: 0.8663\n",
      "Epoch 34: val_loss did not improve from 0.60854\n",
      "219/219 [==============================] - 6s 26ms/step - loss: 0.4290 - accuracy: 0.8663 - val_loss: 0.6144 - val_accuracy: 0.7960 - lr: 9.0000e-06\n",
      "Epoch 35/50\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.4294 - accuracy: 0.8619\n",
      "Epoch 35: val_loss did not improve from 0.60854\n",
      "219/219 [==============================] - 6s 25ms/step - loss: 0.4294 - accuracy: 0.8619 - val_loss: 0.6135 - val_accuracy: 0.7960 - lr: 2.7000e-06\n",
      "Epoch 36/50\n",
      "217/219 [============================>.] - ETA: 0s - loss: 0.4263 - accuracy: 0.8681\n",
      "Epoch 36: val_loss did not improve from 0.60854\n",
      "219/219 [==============================] - 6s 26ms/step - loss: 0.4278 - accuracy: 0.8676 - val_loss: 0.6156 - val_accuracy: 0.7957 - lr: 2.7000e-06\n",
      "Epoch 37/50\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.4298 - accuracy: 0.8664\n",
      "Epoch 37: val_loss did not improve from 0.60854\n",
      "219/219 [==============================] - 5s 25ms/step - loss: 0.4298 - accuracy: 0.8664 - val_loss: 0.6152 - val_accuracy: 0.7947 - lr: 1.0000e-06\n",
      "Epoch 38/50\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.4297 - accuracy: 0.8663\n",
      "Epoch 38: val_loss did not improve from 0.60854\n",
      "219/219 [==============================] - 6s 27ms/step - loss: 0.4297 - accuracy: 0.8663 - val_loss: 0.6146 - val_accuracy: 0.7963 - lr: 1.0000e-06\n",
      "Epoch 39/50\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.4357 - accuracy: 0.8594\n",
      "Epoch 39: val_loss did not improve from 0.60854\n",
      "219/219 [==============================] - 7s 34ms/step - loss: 0.4357 - accuracy: 0.8594 - val_loss: 0.6148 - val_accuracy: 0.7937 - lr: 1.0000e-06\n",
      "Epoch 40/50\n",
      "217/219 [============================>.] - ETA: 0s - loss: 0.4305 - accuracy: 0.8630\n",
      "Epoch 40: val_loss did not improve from 0.60854\n",
      "219/219 [==============================] - 6s 26ms/step - loss: 0.4313 - accuracy: 0.8623 - val_loss: 0.6139 - val_accuracy: 0.7930 - lr: 1.0000e-06\n",
      "Epoch 41/50\n",
      "218/219 [============================>.] - ETA: 0s - loss: 0.4267 - accuracy: 0.8624\n",
      "Epoch 41: val_loss did not improve from 0.60854\n",
      "219/219 [==============================] - 7s 31ms/step - loss: 0.4270 - accuracy: 0.8619 - val_loss: 0.6138 - val_accuracy: 0.7947 - lr: 1.0000e-06\n",
      "Epoch 42/50\n",
      "218/219 [============================>.] - ETA: 0s - loss: 0.4328 - accuracy: 0.8647\n",
      "Epoch 42: val_loss did not improve from 0.60854\n",
      "219/219 [==============================] - 6s 28ms/step - loss: 0.4323 - accuracy: 0.8651 - val_loss: 0.6144 - val_accuracy: 0.7963 - lr: 1.0000e-06\n",
      "Epoch 43/50\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.4267 - accuracy: 0.8639\n",
      "Epoch 43: val_loss did not improve from 0.60854\n",
      "219/219 [==============================] - 5s 25ms/step - loss: 0.4267 - accuracy: 0.8639 - val_loss: 0.6156 - val_accuracy: 0.7957 - lr: 1.0000e-06\n",
      "Epoch 44/50\n",
      "217/219 [============================>.] - ETA: 0s - loss: 0.4314 - accuracy: 0.8623\n",
      "Epoch 44: val_loss did not improve from 0.60854\n",
      "219/219 [==============================] - 5s 24ms/step - loss: 0.4311 - accuracy: 0.8624 - val_loss: 0.6146 - val_accuracy: 0.7933 - lr: 1.0000e-06\n",
      "Epoch 45/50\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.4225 - accuracy: 0.8670\n",
      "Epoch 45: val_loss did not improve from 0.60854\n",
      "219/219 [==============================] - 5s 25ms/step - loss: 0.4225 - accuracy: 0.8670 - val_loss: 0.6153 - val_accuracy: 0.7933 - lr: 1.0000e-06\n",
      "Epoch 46/50\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.4301 - accuracy: 0.8594\n",
      "Epoch 46: val_loss did not improve from 0.60854\n",
      "219/219 [==============================] - 5s 24ms/step - loss: 0.4301 - accuracy: 0.8594 - val_loss: 0.6152 - val_accuracy: 0.7950 - lr: 1.0000e-06\n",
      "Epoch 47/50\n",
      "218/219 [============================>.] - ETA: 0s - loss: 0.4357 - accuracy: 0.8587\n",
      "Epoch 47: val_loss did not improve from 0.60854\n",
      "219/219 [==============================] - 5s 25ms/step - loss: 0.4356 - accuracy: 0.8587 - val_loss: 0.6157 - val_accuracy: 0.7947 - lr: 1.0000e-06\n",
      "Epoch 48/50\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.4256 - accuracy: 0.8639\n",
      "Epoch 48: val_loss did not improve from 0.60854\n",
      "219/219 [==============================] - 6s 26ms/step - loss: 0.4256 - accuracy: 0.8639 - val_loss: 0.6150 - val_accuracy: 0.7953 - lr: 1.0000e-06\n",
      "Epoch 49/50\n",
      "218/219 [============================>.] - ETA: 0s - loss: 0.4347 - accuracy: 0.8581\n",
      "Epoch 49: val_loss did not improve from 0.60854\n",
      "219/219 [==============================] - 6s 29ms/step - loss: 0.4353 - accuracy: 0.8576 - val_loss: 0.6158 - val_accuracy: 0.7937 - lr: 1.0000e-06\n",
      "Epoch 50/50\n",
      "217/219 [============================>.] - ETA: 0s - loss: 0.4269 - accuracy: 0.8668\n",
      "Epoch 50: val_loss did not improve from 0.60854\n",
      "219/219 [==============================] - 6s 28ms/step - loss: 0.4266 - accuracy: 0.8663 - val_loss: 0.6151 - val_accuracy: 0.7957 - lr: 1.0000e-06\n",
      "113/113 [==============================] - 1s 10ms/step - loss: 0.6397 - accuracy: 0.7925\n",
      "Test accuracy: 79.25%\n"
     ]
    }
   ],
   "source": [
    "def get_sequence_model():\n",
    "    class_vocab = label_processor.get_vocabulary()\n",
    "\n",
    "    frame_features_input = keras.Input((MAX_SEQ_LENGTH, NUM_FEATURES))\n",
    "    mask_input = keras.Input((MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "\n",
    "    # Refer to the following tutorial to understand the significance of using `mask`:\n",
    "    # https://keras.io/api/layers/recurrent_layers/gru/\n",
    "    x = keras.layers.GRU(16, return_sequences=True)(\n",
    "        frame_features_input, mask=mask_input\n",
    "    )\n",
    "    x = keras.layers.GRU(8)(x)\n",
    "    x = keras.layers.Dropout(0.4)(x)\n",
    "    x = keras.layers.Dense(8, activation=\"relu\")(x)\n",
    "    output = keras.layers.Dense(len(class_vocab), activation=\"softmax\")(x)\n",
    "\n",
    "    rnn_model = keras.Model([frame_features_input, mask_input], output)\n",
    "\n",
    "    rnn_model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\", optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return rnn_model\n",
    "\n",
    "\n",
    "# Utility for running experiments.\n",
    "def run_experiment():\n",
    "    filepath = \"/workspaces/final-project-shot-type/experiments/inceptionv3_GRU/exp_005/model.weights_exp_005.h5\"\n",
    "    reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n",
    "                              patience=2, min_lr=0.000001)\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "        filepath, save_weights_only=True, save_best_only=True, verbose=1\n",
    "    )\n",
    "\n",
    "    seq_model = get_sequence_model()\n",
    "    history = seq_model.fit(\n",
    "        [train_data[0], train_data[1]],\n",
    "        train_labels,\n",
    "        validation_split=0.3,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=[checkpoint, reduce_lr],\n",
    "    )\n",
    "\n",
    "    seq_model.load_weights(filepath)\n",
    "    _, accuracy = seq_model.evaluate([test_data[0], test_data[1]], test_labels)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "\n",
    "    return history, seq_model\n",
    "\n",
    "\n",
    "_, sequence_model = run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_single_video(frames):\n",
    "    frames = frames[None, ...]\n",
    "    frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "    frame_features = np.zeros(shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\")\n",
    "\n",
    "    for i, batch in enumerate(frames):\n",
    "        video_length = batch.shape[0]\n",
    "        length = min(MAX_SEQ_LENGTH, video_length)\n",
    "        for j in range(length):\n",
    "            frame_features[i, j, :] = feature_extractor.predict(batch[None, j, :])\n",
    "        frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n",
    "\n",
    "    return frame_features, frame_mask\n",
    "\n",
    "\n",
    "def sequence_prediction(path):\n",
    "    class_vocab = label_processor.get_vocabulary()\n",
    "\n",
    "    frames = load_video(os.path.join(\"test\", path))\n",
    "    frame_features, frame_mask = prepare_single_video(frames)\n",
    "    probabilities = sequence_model.predict([frame_features, frame_mask])[0]\n",
    "\n",
    "    for i in np.argsort(probabilities)[::-1]:\n",
    "        print(f\"  {class_vocab[i]}: {probabilities[i] * 100:5.2f}%\")\n",
    "    return frames\n",
    "\n",
    "\n",
    "# This utility is for visualization.\n",
    "# Referenced from:\n",
    "# https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub\n",
    "# def to_gif(images):\n",
    "#     converted_images = images.astype(np.uint8)\n",
    "#     imageio.mimsave(\"animation.gif\", converted_images, fps=10)\n",
    "#     return embed.embed_file(\"animation.gif\")\n",
    "\n",
    "\n",
    "test_video = np.random.choice(test_df[\"video_name\"].values.tolist())\n",
    "print(f\"Test video path: {test_video}\")\n",
    "test_frames = sequence_prediction(test_video)\n",
    "# to_gif(test_frames[:MAX_SEQ_LENGTH])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
