{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tensorflow_docs.vis import embed\n",
    "from tensorflow import keras\n",
    "#from imutils import paths\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import imageio\n",
    "import cv2\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-15 01:37:50.836246: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-15 01:37:50.845618: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-15 01:37:50.846661: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "#Prevent tensorflow to allocate the entire GPU\n",
    "#https://www.tensorflow.org/api_docs/python/tf/config/experimental/set_memory_growth\n",
    "physical_devices = tf.config.list_physical_devices(\"GPU\")\n",
    "try:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df= pd.read_csv('/home/app/src/data/CSV/dataset_train_v2+v3.csv', )\n",
    "# test_df= pd.read_csv('/home/app/src/data/CSV/dataset_test_v3.csv', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df= pd.read_csv('/home/app/src/data/CSV/dataset_train_v2+v3_with_rep.csv')\n",
    "test_df= pd.read_csv('/home/app/src/data/CSV/dataset_test_v2+v3_with_rep.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total videos for training: 29586\n",
      "Total videos for testing: 8149\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_name</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6343</th>\n",
       "      <td>/home/app/src/data/shot-type-dataset/trailer_v...</td>\n",
       "      <td>Push</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16034</th>\n",
       "      <td>/home/app/src/data/shot-type-dataset/trailer_v...</td>\n",
       "      <td>Motion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24850</th>\n",
       "      <td>/home/app/src/data/shot-type-dataset/trailer_v...</td>\n",
       "      <td>Static</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9729</th>\n",
       "      <td>/home/app/src/data/shot-type-dataset/trailer_v...</td>\n",
       "      <td>Static</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27746</th>\n",
       "      <td>/home/app/src/data/shot-type-dataset/trailer_v...</td>\n",
       "      <td>Push</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21390</th>\n",
       "      <td>/home/app/src/data/shot-type-dataset/trailer_v...</td>\n",
       "      <td>Static</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5606</th>\n",
       "      <td>/home/app/src/data/shot-type-dataset/trailer_v...</td>\n",
       "      <td>Motion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13422</th>\n",
       "      <td>/home/app/src/data/shot-type-dataset/trailer_v...</td>\n",
       "      <td>Static</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4278</th>\n",
       "      <td>/home/app/src/data/shot-type-dataset/trailer_v...</td>\n",
       "      <td>Static</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24581</th>\n",
       "      <td>/home/app/src/data/shot-type-dataset/trailer_v...</td>\n",
       "      <td>Push</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              video_name     tag\n",
       "6343   /home/app/src/data/shot-type-dataset/trailer_v...    Push\n",
       "16034  /home/app/src/data/shot-type-dataset/trailer_v...  Motion\n",
       "24850  /home/app/src/data/shot-type-dataset/trailer_v...  Static\n",
       "9729   /home/app/src/data/shot-type-dataset/trailer_v...  Static\n",
       "27746  /home/app/src/data/shot-type-dataset/trailer_v...    Push\n",
       "21390  /home/app/src/data/shot-type-dataset/trailer_v...  Static\n",
       "5606   /home/app/src/data/shot-type-dataset/trailer_v...  Motion\n",
       "13422  /home/app/src/data/shot-type-dataset/trailer_v...  Static\n",
       "4278   /home/app/src/data/shot-type-dataset/trailer_v...  Static\n",
       "24581  /home/app/src/data/shot-type-dataset/trailer_v...    Push"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df= train_df\n",
    "test_df = test_df\n",
    "\n",
    "print(f\"Total videos for training: {len(train_df)}\")\n",
    "print(f\"Total videos for testing: {len(test_df)}\")\n",
    "\n",
    "train_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking correct video tagging with original JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reading the JSON data using json.load()\n",
    "# json_path = '/home/app/src/data/shot-type-dataset/v1_split_trailer.json'\n",
    "# json_v2_path = '/home/app/src/data/shot-type-dataset/v2_full_trailer.json'\n",
    "\n",
    "# with open(json_path) as train_file:\n",
    "#     dict_v1= json.load(train_file)\n",
    "# with open(json_v2_path) as v2_file:\n",
    "#     dict_v2= json.load(v2_file)\n",
    "\n",
    "\n",
    "# def tag_ckecker (df, json_dict1, json_dict2, df_name_to_check):\n",
    "#     tag_chek_list = []\n",
    "\n",
    "#     for i in range(len(df.index)):\n",
    "#         movie = df['video_name'][i]\n",
    "#         df_tag = df['tag'][df['video_name'] == movie][i]\n",
    "#         find_char_st = movie.find('_', 55,60)\n",
    "#         find_char_end = movie.find('_', 65,70)\n",
    "#         movie_key = movie[find_char_st+1:find_char_end]\n",
    "#         trailer_key =  movie[find_char_end+1:find_char_end+5]\n",
    "#         try:\n",
    "#             dict_tag = json_dict1[df_name_to_check][movie_key][trailer_key]['movement']['label']\n",
    "#         except KeyError:\n",
    "#             try:\n",
    "#                 dict_tag = json_dict1['val'][movie_key][trailer_key]['movement']['label']\n",
    "#                 if dict_tag == df_tag:\n",
    "#                     pass\n",
    "#                 else:\n",
    "#                     tag_chek_list.append(movie)\n",
    "#             except KeyError:\n",
    "#                 dict_tag = json_dict2[movie_key][trailer_key]['movement']['label']\n",
    "#                 if dict_tag == df_tag:\n",
    "#                     pass\n",
    "#                 else:\n",
    "#                     tag_chek_list.append(movie)\n",
    "\n",
    "            \n",
    "\n",
    "#     if len(tag_chek_list) == 0:\n",
    "#         print(f'tag check {df_name_to_check} OK')\n",
    "#     else: print (tag_chek_list)\n",
    "\n",
    "\n",
    "# tag_ckecker(train_df, dict_v1, dict_v2, 'train')\n",
    "# tag_ckecker(test_df, dict_v1, dict_v2, 'test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train value counts:\n",
      " Push      530\n",
      "Pull       76\n",
      "Static      2\n",
      "Motion      2\n",
      "Name: tag, dtype: int64 \n",
      "\n",
      "Test value counts:\n",
      " Static            5464\n",
      "Motion            2385\n",
      "Push               220\n",
      "Pull                80\n",
      "Multi_movement      38\n",
      "Name: tag, dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'Train value counts:' '\\n',train_df['tag'].value_counts(), '\\n')\n",
    "print(f'Test value counts:' '\\n',test_df['tag'].value_counts(), '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 40\n",
    "\n",
    "MAX_SEQ_LENGTH = 20\n",
    "NUM_FEATURES = 2048"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading frames and working on frame's size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_center_square(frame):\n",
    "    y, x = frame.shape[0:2]\n",
    "    min_dim = min(y, x)\n",
    "    start_x = (x // 2) - (min_dim // 2)\n",
    "    start_y = (y // 2) - (min_dim // 2)\n",
    "    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]\n",
    "\n",
    "\n",
    "def load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = []\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = crop_center_square(frame)\n",
    "            frame = cv2.resize(frame, resize)\n",
    "            frame = frame[:, :, [2, 1, 0]]\n",
    "            frames.append(frame)\n",
    "\n",
    "            if len(frames) == max_frames:\n",
    "                break\n",
    "    finally:\n",
    "        cap.release()\n",
    "    return np.array(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-15 00:04:21.453274: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-15 00:04:21.453837: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-15 00:04:21.454884: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-15 00:04:21.455855: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-15 00:04:22.130750: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-15 00:04:22.131594: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-15 00:04:22.132439: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-15 00:04:22.133200: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10794 MB memory:  -> device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "87916544/87910968 [==============================] - 2s 0us/step\n",
      "87924736/87910968 [==============================] - 2s 0us/step\n",
      "Model: \"feature_extractor\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " tf.math.truediv (TFOpLambda  (None, 224, 224, 3)      0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " tf.math.subtract (TFOpLambd  (None, 224, 224, 3)      0         \n",
      " a)                                                              \n",
      "                                                                 \n",
      " inception_v3 (Functional)   (None, 2048)              21802784  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21,802,784\n",
      "Trainable params: 21,768,352\n",
      "Non-trainable params: 34,432\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def build_feature_extractor():\n",
    "    feature_extractor = keras.applications.InceptionV3(\n",
    "        weights=\"imagenet\",\n",
    "        include_top=False,\n",
    "        pooling=\"avg\",\n",
    "        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "    )\n",
    "    preprocess_input = keras.applications.inception_v3.preprocess_input\n",
    "\n",
    "    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
    "    preprocessed = preprocess_input(inputs)\n",
    "\n",
    "    outputs = feature_extractor(preprocessed)\n",
    "    return keras.Model(inputs, outputs, name=\"feature_extractor\")\n",
    "\n",
    "\n",
    "feature_extractor = build_feature_extractor()\n",
    "print(feature_extractor.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working on labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Motion', 'Pull', 'Push', 'Static']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-15 01:38:20.787486: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-15 01:38:20.788115: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-15 01:38:20.789110: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-15 01:38:20.789917: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-15 01:38:21.440694: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-15 01:38:21.441536: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-15 01:38:21.442341: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-15 01:38:21.443078: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10794 MB memory:  -> device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7\n"
     ]
    }
   ],
   "source": [
    "label_processor = keras.layers.StringLookup(\n",
    "    num_oov_indices=0, vocabulary=np.unique(train_df[\"tag\"])\n",
    ")\n",
    "print(label_processor.get_vocabulary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-15 00:05:39.055568: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8200\n",
      "2022-11-15 00:05:39.329045: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-11-15 00:05:39.329756: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-11-15 00:05:39.329810: W tensorflow/stream_executor/gpu/asm_compiler.cc:80] Couldn't get ptxas version string: INTERNAL: Couldn't invoke ptxas --version\n",
      "2022-11-15 00:05:39.330398: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-11-15 00:05:39.330505: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] INTERNAL: Failed to launch ptxas\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " train_video 0/610 done\n",
      " train_video 1/610 done\n",
      " train_video 2/610 done\n",
      " train_video 3/610 done\n",
      " train_video 4/610 done\n",
      " train_video 5/610 done\n",
      " train_video 6/610 done\n",
      " train_video 7/610 done\n",
      " train_video 8/610 done\n",
      " train_video 9/610 done\n",
      " train_video 10/610 done\n",
      " train_video 11/610 done\n",
      " train_video 12/610 done\n",
      " train_video 13/610 done\n",
      " train_video 14/610 done\n",
      " train_video 15/610 done\n",
      " train_video 16/610 done\n",
      " train_video 17/610 done\n",
      " train_video 18/610 done\n",
      " train_video 19/610 done\n",
      " train_video 20/610 done\n",
      " train_video 21/610 done\n",
      " train_video 22/610 done\n",
      " train_video 23/610 done\n",
      " train_video 24/610 done\n",
      " train_video 25/610 done\n",
      " train_video 26/610 done\n",
      " train_video 27/610 done\n",
      " train_video 28/610 done\n",
      " train_video 29/610 done\n",
      " train_video 30/610 done\n",
      " train_video 31/610 done\n",
      " train_video 32/610 done\n",
      " train_video 33/610 done\n",
      " train_video 34/610 done\n",
      " train_video 35/610 done\n",
      " train_video 36/610 done\n",
      " train_video 37/610 done\n",
      " train_video 38/610 done\n",
      " train_video 39/610 done\n",
      " train_video 40/610 done\n",
      " train_video 41/610 done\n",
      " train_video 42/610 done\n",
      " train_video 43/610 done\n",
      " train_video 44/610 done\n",
      " train_video 45/610 done\n",
      " train_video 46/610 done\n",
      " train_video 47/610 done\n",
      " train_video 48/610 done\n",
      " train_video 49/610 done\n",
      " train_video 50/610 done\n",
      " train_video 51/610 done\n",
      " train_video 52/610 done\n",
      " train_video 53/610 done\n",
      " train_video 54/610 done\n",
      " train_video 55/610 done\n",
      " train_video 56/610 done\n",
      " train_video 57/610 done\n",
      " train_video 58/610 done\n",
      " train_video 59/610 done\n",
      " train_video 60/610 done\n",
      " train_video 61/610 done\n",
      " train_video 62/610 done\n",
      " train_video 63/610 done\n",
      " train_video 64/610 done\n",
      " train_video 65/610 done\n",
      " train_video 66/610 done\n",
      " train_video 67/610 done\n",
      " train_video 68/610 done\n",
      " train_video 69/610 done\n",
      " train_video 70/610 done\n",
      " train_video 71/610 done\n",
      " train_video 72/610 done\n",
      " train_video 73/610 done\n",
      " train_video 74/610 done\n",
      " train_video 75/610 done\n",
      " train_video 76/610 done\n",
      " train_video 77/610 done\n",
      " train_video 78/610 done\n",
      " train_video 79/610 done\n",
      " train_video 80/610 done\n",
      " train_video 81/610 done\n",
      " train_video 82/610 done\n",
      " train_video 83/610 done\n",
      " train_video 84/610 done\n",
      " train_video 85/610 done\n",
      " train_video 86/610 done\n",
      " train_video 87/610 done\n",
      " train_video 88/610 done\n",
      " train_video 89/610 done\n",
      " train_video 90/610 done\n",
      " train_video 91/610 done\n",
      " train_video 92/610 done\n",
      " train_video 93/610 done\n",
      " train_video 94/610 done\n",
      " train_video 95/610 done\n",
      " train_video 96/610 done\n",
      " train_video 97/610 done\n",
      " train_video 98/610 done\n",
      " train_video 99/610 done\n",
      " train_video 100/610 done\n",
      " train_video 101/610 done\n",
      " train_video 102/610 done\n",
      " train_video 103/610 done\n",
      " train_video 104/610 done\n",
      " train_video 105/610 done\n",
      " train_video 106/610 done\n",
      " train_video 107/610 done\n",
      " train_video 108/610 done\n",
      " train_video 109/610 done\n",
      " train_video 110/610 done\n",
      " train_video 111/610 done\n",
      " train_video 112/610 done\n",
      " train_video 113/610 done\n",
      " train_video 114/610 done\n",
      " train_video 115/610 done\n",
      " train_video 116/610 done\n",
      " train_video 117/610 done\n",
      " train_video 118/610 done\n",
      " train_video 119/610 done\n",
      " train_video 120/610 done\n",
      " train_video 121/610 done\n",
      " train_video 122/610 done\n",
      " train_video 123/610 done\n",
      " train_video 124/610 done\n",
      " train_video 125/610 done\n",
      " train_video 126/610 done\n",
      " train_video 127/610 done\n",
      " train_video 128/610 done\n",
      " train_video 129/610 done\n",
      " train_video 130/610 done\n",
      " train_video 131/610 done\n",
      " train_video 132/610 done\n",
      " train_video 133/610 done\n",
      " train_video 134/610 done\n",
      " train_video 135/610 done\n",
      " train_video 136/610 done\n",
      " train_video 137/610 done\n",
      " train_video 138/610 done\n",
      " train_video 139/610 done\n",
      " train_video 140/610 done\n",
      " train_video 141/610 done\n",
      " train_video 142/610 done\n",
      " train_video 143/610 done\n",
      " train_video 144/610 done\n",
      " train_video 145/610 done\n",
      " train_video 146/610 done\n",
      " train_video 147/610 done\n",
      " train_video 148/610 done\n",
      " train_video 149/610 done\n",
      " train_video 150/610 done\n",
      " train_video 151/610 done\n",
      " train_video 152/610 done\n",
      " train_video 153/610 done\n",
      " train_video 154/610 done\n",
      " train_video 155/610 done\n",
      " train_video 156/610 done\n",
      " train_video 157/610 done\n",
      " train_video 158/610 done\n",
      " train_video 159/610 done\n",
      " train_video 160/610 done\n",
      " train_video 161/610 done\n",
      " train_video 162/610 done\n",
      " train_video 163/610 done\n",
      " train_video 164/610 done\n",
      " train_video 165/610 done\n",
      " train_video 166/610 done\n",
      " train_video 167/610 done\n",
      " train_video 168/610 done\n",
      " train_video 169/610 done\n",
      " train_video 170/610 done\n",
      " train_video 171/610 done\n",
      " train_video 172/610 done\n",
      " train_video 173/610 done\n",
      " train_video 174/610 done\n",
      " train_video 175/610 done\n",
      " train_video 176/610 done\n",
      " train_video 177/610 done\n",
      " train_video 178/610 done\n",
      " train_video 179/610 done\n",
      " train_video 180/610 done\n",
      " train_video 181/610 done\n",
      " train_video 182/610 done\n",
      " train_video 183/610 done\n",
      " train_video 184/610 done\n",
      " train_video 185/610 done\n",
      " train_video 186/610 done\n",
      " train_video 187/610 done\n",
      " train_video 188/610 done\n",
      " train_video 189/610 done\n",
      " train_video 190/610 done\n",
      " train_video 191/610 done\n",
      " train_video 192/610 done\n",
      " train_video 193/610 done\n",
      " train_video 194/610 done\n",
      " train_video 195/610 done\n",
      " train_video 196/610 done\n",
      " train_video 197/610 done\n",
      " train_video 198/610 done\n",
      " train_video 199/610 done\n",
      " train_video 200/610 done\n",
      " train_video 201/610 done\n",
      " train_video 202/610 done\n",
      " train_video 203/610 done\n",
      " train_video 204/610 done\n",
      " train_video 205/610 done\n",
      " train_video 206/610 done\n",
      " train_video 207/610 done\n",
      " train_video 208/610 done\n",
      " train_video 209/610 done\n",
      " train_video 210/610 done\n",
      " train_video 211/610 done\n",
      " train_video 212/610 done\n",
      " train_video 213/610 done\n",
      " train_video 214/610 done\n",
      " train_video 215/610 done\n",
      " train_video 216/610 done\n",
      " train_video 217/610 done\n",
      " train_video 218/610 done\n",
      " train_video 219/610 done\n",
      " train_video 220/610 done\n",
      " train_video 221/610 done\n",
      " train_video 222/610 done\n",
      " train_video 223/610 done\n",
      " train_video 224/610 done\n",
      " train_video 225/610 done\n",
      " train_video 226/610 done\n",
      " train_video 227/610 done\n",
      " train_video 228/610 done\n",
      " train_video 229/610 done\n",
      " train_video 230/610 done\n",
      " train_video 231/610 done\n",
      " train_video 232/610 done\n",
      " train_video 233/610 done\n",
      " train_video 234/610 done\n",
      " train_video 235/610 done\n",
      " train_video 236/610 done\n",
      " train_video 237/610 done\n",
      " train_video 238/610 done\n",
      " train_video 239/610 done\n",
      " train_video 240/610 done\n",
      " train_video 241/610 done\n",
      " train_video 242/610 done\n",
      " train_video 243/610 done\n",
      " train_video 244/610 done\n",
      " train_video 245/610 done\n",
      " train_video 246/610 done\n",
      " train_video 247/610 done\n",
      " train_video 248/610 done\n",
      " train_video 249/610 done\n",
      " train_video 250/610 done\n",
      " train_video 251/610 done\n",
      " train_video 252/610 done\n",
      " train_video 253/610 done\n",
      " train_video 254/610 done\n",
      " train_video 255/610 done\n",
      " train_video 256/610 done\n",
      " train_video 257/610 done\n",
      " train_video 258/610 done\n",
      " train_video 259/610 done\n",
      " train_video 260/610 done\n",
      " train_video 261/610 done\n",
      " train_video 262/610 done\n",
      " train_video 263/610 done\n",
      " train_video 264/610 done\n",
      " train_video 265/610 done\n",
      " train_video 266/610 done\n",
      " train_video 267/610 done\n",
      " train_video 268/610 done\n",
      " train_video 269/610 done\n",
      " train_video 270/610 done\n",
      " train_video 271/610 done\n",
      " train_video 272/610 done\n",
      " train_video 273/610 done\n",
      " train_video 274/610 done\n",
      " train_video 275/610 done\n",
      " train_video 276/610 done\n",
      " train_video 277/610 done\n",
      " train_video 278/610 done\n",
      " train_video 279/610 done\n",
      " train_video 280/610 done\n",
      " train_video 281/610 done\n",
      " train_video 282/610 done\n",
      " train_video 283/610 done\n",
      " train_video 284/610 done\n",
      " train_video 285/610 done\n",
      " train_video 286/610 done\n",
      " train_video 287/610 done\n",
      " train_video 288/610 done\n",
      " train_video 289/610 done\n",
      " train_video 290/610 done\n",
      " train_video 291/610 done\n",
      " train_video 292/610 done\n",
      " train_video 293/610 done\n",
      " train_video 294/610 done\n",
      " train_video 295/610 done\n",
      " train_video 296/610 done\n",
      " train_video 297/610 done\n",
      " train_video 298/610 done\n",
      " train_video 299/610 done\n",
      " train_video 300/610 done\n",
      " train_video 301/610 done\n",
      " train_video 302/610 done\n",
      " train_video 303/610 done\n",
      " train_video 304/610 done\n",
      " train_video 305/610 done\n",
      " train_video 306/610 done\n",
      " train_video 307/610 done\n",
      " train_video 308/610 done\n",
      " train_video 309/610 done\n",
      " train_video 310/610 done\n",
      " train_video 311/610 done\n",
      " train_video 312/610 done\n",
      " train_video 313/610 done\n",
      " train_video 314/610 done\n",
      " train_video 315/610 done\n",
      " train_video 316/610 done\n",
      " train_video 317/610 done\n",
      " train_video 318/610 done\n",
      " train_video 319/610 done\n",
      " train_video 320/610 done\n",
      " train_video 321/610 done\n",
      " train_video 322/610 done\n",
      " train_video 323/610 done\n",
      " train_video 324/610 done\n",
      " train_video 325/610 done\n",
      " train_video 326/610 done\n",
      " train_video 327/610 done\n",
      " train_video 328/610 done\n",
      " train_video 329/610 done\n",
      " train_video 330/610 done\n",
      " train_video 331/610 done\n",
      " train_video 332/610 done\n",
      " train_video 333/610 done\n",
      " train_video 334/610 done\n",
      " train_video 335/610 done\n",
      " train_video 336/610 done\n",
      " train_video 337/610 done\n",
      " train_video 338/610 done\n",
      " train_video 339/610 done\n",
      " train_video 340/610 done\n",
      " train_video 341/610 done\n",
      " train_video 342/610 done\n",
      " train_video 343/610 done\n",
      " train_video 344/610 done\n",
      " train_video 345/610 done\n",
      " train_video 346/610 done\n",
      " train_video 347/610 done\n",
      " train_video 348/610 done\n",
      " train_video 349/610 done\n",
      " train_video 350/610 done\n",
      " train_video 351/610 done\n",
      " train_video 352/610 done\n",
      " train_video 353/610 done\n",
      " train_video 354/610 done\n",
      " train_video 355/610 done\n",
      " train_video 356/610 done\n",
      " train_video 357/610 done\n",
      " train_video 358/610 done\n",
      " train_video 359/610 done\n",
      " train_video 360/610 done\n",
      " train_video 361/610 done\n",
      " train_video 362/610 done\n",
      " train_video 363/610 done\n",
      " train_video 364/610 done\n",
      " train_video 365/610 done\n",
      " train_video 366/610 done\n",
      " train_video 367/610 done\n",
      " train_video 368/610 done\n",
      " train_video 369/610 done\n",
      " train_video 370/610 done\n",
      " train_video 371/610 done\n",
      " train_video 372/610 done\n",
      " train_video 373/610 done\n",
      " train_video 374/610 done\n",
      " train_video 375/610 done\n",
      " train_video 376/610 done\n",
      " train_video 377/610 done\n",
      " train_video 378/610 done\n",
      " train_video 379/610 done\n",
      " train_video 380/610 done\n",
      " train_video 381/610 done\n",
      " train_video 382/610 done\n",
      " train_video 383/610 done\n",
      " train_video 384/610 done\n",
      " train_video 385/610 done\n",
      " train_video 386/610 done\n",
      " train_video 387/610 done\n",
      " train_video 388/610 done\n",
      " train_video 389/610 done\n",
      " train_video 390/610 done\n",
      " train_video 391/610 done\n",
      " train_video 392/610 done\n",
      " train_video 393/610 done\n",
      " train_video 394/610 done\n",
      " train_video 395/610 done\n",
      " train_video 396/610 done\n",
      " train_video 397/610 done\n",
      " train_video 398/610 done\n",
      " train_video 399/610 done\n",
      " train_video 400/610 done\n",
      " train_video 401/610 done\n",
      " train_video 402/610 done\n",
      " train_video 403/610 done\n",
      " train_video 404/610 done\n",
      " train_video 405/610 done\n",
      " train_video 406/610 done\n",
      " train_video 407/610 done\n",
      " train_video 408/610 done\n",
      " train_video 409/610 done\n",
      " train_video 410/610 done\n",
      " train_video 411/610 done\n",
      " train_video 412/610 done\n",
      " train_video 413/610 done\n",
      " train_video 414/610 done\n",
      " train_video 415/610 done\n",
      " train_video 416/610 done\n",
      " train_video 417/610 done\n",
      " train_video 418/610 done\n",
      " train_video 419/610 done\n",
      " train_video 420/610 done\n",
      " train_video 421/610 done\n",
      " train_video 422/610 done\n",
      " train_video 423/610 done\n",
      " train_video 424/610 done\n",
      " train_video 425/610 done\n",
      " train_video 426/610 done\n",
      " train_video 427/610 done\n",
      " train_video 428/610 done\n",
      " train_video 429/610 done\n",
      " train_video 430/610 done\n",
      " train_video 431/610 done\n",
      " train_video 432/610 done\n",
      " train_video 433/610 done\n",
      " train_video 434/610 done\n",
      " train_video 435/610 done\n",
      " train_video 436/610 done\n",
      " train_video 437/610 done\n",
      " train_video 438/610 done\n",
      " train_video 439/610 done\n",
      " train_video 440/610 done\n",
      " train_video 441/610 done\n",
      " train_video 442/610 done\n",
      " train_video 443/610 done\n",
      " train_video 444/610 done\n",
      " train_video 445/610 done\n",
      " train_video 446/610 done\n",
      " train_video 447/610 done\n",
      " train_video 448/610 done\n",
      " train_video 449/610 done\n",
      " train_video 450/610 done\n",
      " train_video 451/610 done\n",
      " train_video 452/610 done\n",
      " train_video 453/610 done\n",
      " train_video 454/610 done\n",
      " train_video 455/610 done\n",
      " train_video 456/610 done\n",
      " train_video 457/610 done\n",
      " train_video 458/610 done\n",
      " train_video 459/610 done\n",
      " train_video 460/610 done\n",
      " train_video 461/610 done\n",
      " train_video 462/610 done\n",
      " train_video 463/610 done\n",
      " train_video 464/610 done\n",
      " train_video 465/610 done\n",
      " train_video 466/610 done\n",
      " train_video 467/610 done\n",
      " train_video 468/610 done\n",
      " train_video 469/610 done\n",
      " train_video 470/610 done\n",
      " train_video 471/610 done\n",
      " train_video 472/610 done\n",
      " train_video 473/610 done\n",
      " train_video 474/610 done\n",
      " train_video 475/610 done\n",
      " train_video 476/610 done\n",
      " train_video 477/610 done\n",
      " train_video 478/610 done\n",
      " train_video 479/610 done\n",
      " train_video 480/610 done\n",
      " train_video 481/610 done\n",
      " train_video 482/610 done\n",
      " train_video 483/610 done\n",
      " train_video 484/610 done\n",
      " train_video 485/610 done\n",
      " train_video 486/610 done\n",
      " train_video 487/610 done\n",
      " train_video 488/610 done\n",
      " train_video 489/610 done\n",
      " train_video 490/610 done\n",
      " train_video 491/610 done\n",
      " train_video 492/610 done\n",
      " train_video 493/610 done\n",
      " train_video 494/610 done\n",
      " train_video 495/610 done\n",
      " train_video 496/610 done\n",
      " train_video 497/610 done\n",
      " train_video 498/610 done\n",
      " train_video 499/610 done\n",
      " train_video 500/610 done\n",
      " train_video 501/610 done\n",
      " train_video 502/610 done\n",
      " train_video 503/610 done\n",
      " train_video 504/610 done\n",
      " train_video 505/610 done\n",
      " train_video 506/610 done\n",
      " train_video 507/610 done\n",
      " train_video 508/610 done\n",
      " train_video 509/610 done\n",
      " train_video 510/610 done\n",
      " train_video 511/610 done\n",
      " train_video 512/610 done\n",
      " train_video 513/610 done\n",
      " train_video 514/610 done\n",
      " train_video 515/610 done\n",
      " train_video 516/610 done\n",
      " train_video 517/610 done\n",
      " train_video 518/610 done\n",
      " train_video 519/610 done\n",
      " train_video 520/610 done\n",
      " train_video 521/610 done\n",
      " train_video 522/610 done\n",
      " train_video 523/610 done\n",
      " train_video 524/610 done\n",
      " train_video 525/610 done\n",
      " train_video 526/610 done\n",
      " train_video 527/610 done\n",
      " train_video 528/610 done\n",
      " train_video 529/610 done\n",
      " train_video 530/610 done\n",
      " train_video 531/610 done\n",
      " train_video 532/610 done\n",
      " train_video 533/610 done\n",
      " train_video 534/610 done\n",
      " train_video 535/610 done\n",
      " train_video 536/610 done\n",
      " train_video 537/610 done\n",
      " train_video 538/610 done\n",
      " train_video 539/610 done\n",
      " train_video 540/610 done\n",
      " train_video 541/610 done\n",
      " train_video 542/610 done\n",
      " train_video 543/610 done\n",
      " train_video 544/610 done\n",
      " train_video 545/610 done\n",
      " train_video 546/610 done\n",
      " train_video 547/610 done\n",
      " train_video 548/610 done\n",
      " train_video 549/610 done\n",
      " train_video 550/610 done\n",
      " train_video 551/610 done\n",
      " train_video 552/610 done\n",
      " train_video 553/610 done\n",
      " train_video 554/610 done\n",
      " train_video 555/610 done\n",
      " train_video 556/610 done\n",
      " train_video 557/610 done\n",
      " train_video 558/610 done\n",
      " train_video 559/610 done\n",
      " train_video 560/610 done\n",
      " train_video 561/610 done\n",
      " train_video 562/610 done\n",
      " train_video 563/610 done\n",
      " train_video 564/610 done\n",
      " train_video 565/610 done\n",
      " train_video 566/610 done\n",
      " train_video 567/610 done\n",
      " train_video 568/610 done\n",
      " train_video 569/610 done\n",
      " train_video 570/610 done\n",
      " train_video 571/610 done\n",
      " train_video 572/610 done\n",
      " train_video 573/610 done\n",
      " train_video 574/610 done\n",
      " train_video 575/610 done\n",
      " train_video 576/610 done\n",
      " train_video 577/610 done\n",
      " train_video 578/610 done\n",
      " train_video 579/610 done\n",
      " train_video 580/610 done\n",
      " train_video 581/610 done\n",
      " train_video 582/610 done\n",
      " train_video 583/610 done\n",
      " train_video 584/610 done\n",
      " train_video 585/610 done\n",
      " train_video 586/610 done\n",
      " train_video 587/610 done\n",
      " train_video 588/610 done\n",
      " train_video 589/610 done\n",
      " train_video 590/610 done\n",
      " train_video 591/610 done\n",
      " train_video 592/610 done\n",
      " train_video 593/610 done\n",
      " train_video 594/610 done\n",
      " train_video 595/610 done\n",
      " train_video 596/610 done\n",
      " train_video 597/610 done\n",
      " train_video 598/610 done\n",
      " train_video 599/610 done\n",
      " train_video 600/610 done\n",
      " train_video 601/610 done\n",
      " train_video 602/610 done\n",
      " train_video 603/610 done\n",
      " train_video 604/610 done\n",
      " train_video 605/610 done\n",
      " train_video 606/610 done\n",
      " train_video 607/610 done\n",
      " train_video 608/610 done\n",
      " train_video 609/610 done\n",
      "Frame features in train set: (610, 10, 2048)\n"
     ]
    }
   ],
   "source": [
    "def prepare_all_videos(df, root_dir):\n",
    "    num_samples = len(df)\n",
    "    video_paths = df[\"video_name\"].values.tolist()\n",
    "    labels = df[\"tag\"].values\n",
    "    labels = label_processor(labels[..., None]).numpy()\n",
    "\n",
    "    # `frame_masks` and `frame_features` are what we will feed to our sequence model.\n",
    "    # `frame_masks` will contain a bunch of booleans denoting if a timestep is\n",
    "    # masked with padding or not.\n",
    "    frame_masks = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH), dtype=\"bool\")\n",
    "    frame_features = np.zeros(\n",
    "        shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
    "    )\n",
    "\n",
    "    # For each video.\n",
    "    for idx, path in enumerate(video_paths):\n",
    "        # Gather all its frames and add a batch dimension.\n",
    "        frames = load_video(os.path.join(root_dir, path))\n",
    "        frames = frames[None, ...]\n",
    "\n",
    "        # Initialize placeholders to store the masks and features of the current video.\n",
    "        temp_frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "        temp_frame_features = np.zeros(\n",
    "            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
    "        )\n",
    "\n",
    "        # Extract features from the frames of the current video.\n",
    "        for i, batch in enumerate(frames):\n",
    "            video_length = batch.shape[0]\n",
    "            length = min(MAX_SEQ_LENGTH, video_length)\n",
    "            for j in range(length):\n",
    "                temp_frame_features[i, j, :] = feature_extractor.predict(\n",
    "                    batch[None, j, :]\n",
    "                )\n",
    "            temp_frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n",
    "\n",
    "        frame_features[idx,] = temp_frame_features.squeeze()\n",
    "        frame_masks[idx,] = temp_frame_mask.squeeze()\n",
    "        print(f' {root_dir}_video {idx}/{len(video_paths)} done')\n",
    "\n",
    "    return (frame_features, frame_masks), labels\n",
    "\n",
    "\n",
    "train_data, train_labels = prepare_all_videos(train_df, \"train\")\n",
    "print(f\"Frame features in train set: {train_data[0].shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving TRAIN embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "path_save_embeddings= '/home/app/src/embeddings/mov_v2_410_movies/'\n",
    "\n",
    "# Train\n",
    "train_data_embedding_0 = np.save(path_save_embeddings + 'train_data_embedding_0.npy', train_data[0])\n",
    "train_data_embedding_1 = np.save(path_save_embeddings + 'train_data_embedding_1.npy', train_data[1])\n",
    "train_labels_embedding = np.save(path_save_embeddings + 'train_labels_embedding.npy', train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing TEST videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data, test_labels = prepare_all_videos(test_df, \"test\")\n",
    "# print(f\"Frame masks in test set: {test_data[1].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving TEST embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Test\n",
    "# test_data_embedding_0 = np.save(path_save_embeddings + 'test_data_embedding_0.npy', test_data[0])\n",
    "# test_data_embedding_1 = np.save(path_save_embeddings + 'test_data_embedding_1.npy', test_data[1])\n",
    "# test_labels_embedding = np.save(path_save_embeddings + 'test_labels_embedding.npy', test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_load_embeddings= '/home/app/src/embeddings/mov_20856_train_8187_test_emb/20856_train_8187_test_10_frames/'\n",
    "\n",
    "# #Train\n",
    "# train_data_0 = np.load(path_load_embeddings + 'train_data_embedding_0_complete_10_frames.npy')\n",
    "# train_data_1 = np.load(path_load_embeddings + 'train_data_embedding_1_complete_10_frames.npy')\n",
    "# train_data=(train_data_0, train_data_1)\n",
    "# #train_labels = np.load(path_load_embeddings + 'train_labels_embedding.npy')\n",
    "\n",
    "# #Test\n",
    "# test_data_0 = np.load(path_load_embeddings + 'test_data_embedding_0_complete_10_frames.npy')\n",
    "# test_data_1 = np.load(path_load_embeddings + 'test_data_embedding_1_complete_10_frames.npy')\n",
    "# test_data=(test_data_0, test_data_1)\n",
    "# #test_labels = np.load(path_load_embeddings + 'test_labels_embedding.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_load_labels= '/home/app/src/embeddings/aa/'\n",
    "train_labels = np.load(path_load_labels + 'train_labels_embedding.npy')\n",
    "test_labels = np.load(path_load_labels + 'test_labels_embedding.npy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequence model (RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-14 22:15:40.058485: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1195950080 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "457/457 [==============================] - ETA: 0s - loss: 1.3998 - accuracy: 0.3506\n",
      "Epoch 1: val_loss improved from inf to 1.20347, saving model to /home/app/src/experiments/inceptionv3_GRU/exp_006/model.weights_exp_005.h5\n",
      "457/457 [==============================] - 21s 27ms/step - loss: 1.3998 - accuracy: 0.3506 - val_loss: 1.2035 - val_accuracy: 0.6537 - lr: 1.0000e-04\n",
      "Epoch 2/40\n",
      "455/457 [============================>.] - ETA: 0s - loss: 1.1203 - accuracy: 0.5953\n",
      "Epoch 2: val_loss improved from 1.20347 to 0.91868, saving model to /home/app/src/experiments/inceptionv3_GRU/exp_006/model.weights_exp_005.h5\n",
      "457/457 [==============================] - 9s 19ms/step - loss: 1.1205 - accuracy: 0.5952 - val_loss: 0.9187 - val_accuracy: 0.6625 - lr: 1.0000e-04\n",
      "Epoch 3/40\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.9487 - accuracy: 0.6432\n",
      "Epoch 3: val_loss improved from 0.91868 to 0.82405, saving model to /home/app/src/experiments/inceptionv3_GRU/exp_006/model.weights_exp_005.h5\n",
      "457/457 [==============================] - 9s 19ms/step - loss: 0.9487 - accuracy: 0.6432 - val_loss: 0.8240 - val_accuracy: 0.6641 - lr: 1.0000e-04\n",
      "Epoch 4/40\n",
      "455/457 [============================>.] - ETA: 0s - loss: 0.8856 - accuracy: 0.6574\n",
      "Epoch 4: val_loss improved from 0.82405 to 0.79296, saving model to /home/app/src/experiments/inceptionv3_GRU/exp_006/model.weights_exp_005.h5\n",
      "457/457 [==============================] - 9s 19ms/step - loss: 0.8856 - accuracy: 0.6574 - val_loss: 0.7930 - val_accuracy: 0.6684 - lr: 1.0000e-04\n",
      "Epoch 5/40\n",
      "456/457 [============================>.] - ETA: 0s - loss: 0.8486 - accuracy: 0.6590\n",
      "Epoch 5: val_loss improved from 0.79296 to 0.78090, saving model to /home/app/src/experiments/inceptionv3_GRU/exp_006/model.weights_exp_005.h5\n",
      "457/457 [==============================] - 8s 19ms/step - loss: 0.8486 - accuracy: 0.6589 - val_loss: 0.7809 - val_accuracy: 0.6724 - lr: 1.0000e-04\n",
      "Epoch 6/40\n",
      "455/457 [============================>.] - ETA: 0s - loss: 0.8251 - accuracy: 0.6626\n",
      "Epoch 6: val_loss improved from 0.78090 to 0.76737, saving model to /home/app/src/experiments/inceptionv3_GRU/exp_006/model.weights_exp_005.h5\n",
      "457/457 [==============================] - 9s 19ms/step - loss: 0.8251 - accuracy: 0.6624 - val_loss: 0.7674 - val_accuracy: 0.6748 - lr: 1.0000e-04\n",
      "Epoch 7/40\n",
      "454/457 [============================>.] - ETA: 0s - loss: 0.8136 - accuracy: 0.6669\n",
      "Epoch 7: val_loss improved from 0.76737 to 0.76269, saving model to /home/app/src/experiments/inceptionv3_GRU/exp_006/model.weights_exp_005.h5\n",
      "457/457 [==============================] - 9s 19ms/step - loss: 0.8134 - accuracy: 0.6672 - val_loss: 0.7627 - val_accuracy: 0.6716 - lr: 1.0000e-04\n",
      "Epoch 8/40\n",
      "454/457 [============================>.] - ETA: 0s - loss: 0.7993 - accuracy: 0.6726\n",
      "Epoch 8: val_loss improved from 0.76269 to 0.75985, saving model to /home/app/src/experiments/inceptionv3_GRU/exp_006/model.weights_exp_005.h5\n",
      "457/457 [==============================] - 9s 19ms/step - loss: 0.7991 - accuracy: 0.6727 - val_loss: 0.7598 - val_accuracy: 0.6751 - lr: 1.0000e-04\n",
      "Epoch 9/40\n",
      "454/457 [============================>.] - ETA: 0s - loss: 0.7857 - accuracy: 0.6785\n",
      "Epoch 9: val_loss improved from 0.75985 to 0.75415, saving model to /home/app/src/experiments/inceptionv3_GRU/exp_006/model.weights_exp_005.h5\n",
      "457/457 [==============================] - 9s 19ms/step - loss: 0.7868 - accuracy: 0.6781 - val_loss: 0.7542 - val_accuracy: 0.6749 - lr: 1.0000e-04\n",
      "Epoch 10/40\n",
      "454/457 [============================>.] - ETA: 0s - loss: 0.7759 - accuracy: 0.6852\n",
      "Epoch 10: val_loss did not improve from 0.75415\n",
      "457/457 [==============================] - 8s 18ms/step - loss: 0.7764 - accuracy: 0.6850 - val_loss: 0.7555 - val_accuracy: 0.6762 - lr: 1.0000e-04\n",
      "Epoch 11/40\n",
      "455/457 [============================>.] - ETA: 0s - loss: 0.7680 - accuracy: 0.6876\n",
      "Epoch 11: val_loss improved from 0.75415 to 0.75130, saving model to /home/app/src/experiments/inceptionv3_GRU/exp_006/model.weights_exp_005.h5\n",
      "457/457 [==============================] - 8s 18ms/step - loss: 0.7675 - accuracy: 0.6880 - val_loss: 0.7513 - val_accuracy: 0.6794 - lr: 1.0000e-04\n",
      "Epoch 12/40\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.7573 - accuracy: 0.6926\n",
      "Epoch 12: val_loss did not improve from 0.75130\n",
      "457/457 [==============================] - 8s 18ms/step - loss: 0.7573 - accuracy: 0.6926 - val_loss: 0.7518 - val_accuracy: 0.6791 - lr: 1.0000e-04\n",
      "Epoch 13/40\n",
      "454/457 [============================>.] - ETA: 0s - loss: 0.7504 - accuracy: 0.6929\n",
      "Epoch 13: val_loss improved from 0.75130 to 0.74989, saving model to /home/app/src/experiments/inceptionv3_GRU/exp_006/model.weights_exp_005.h5\n",
      "457/457 [==============================] - 8s 18ms/step - loss: 0.7508 - accuracy: 0.6929 - val_loss: 0.7499 - val_accuracy: 0.6778 - lr: 1.0000e-04\n",
      "Epoch 14/40\n",
      "455/457 [============================>.] - ETA: 0s - loss: 0.7475 - accuracy: 0.7006\n",
      "Epoch 14: val_loss improved from 0.74989 to 0.74934, saving model to /home/app/src/experiments/inceptionv3_GRU/exp_006/model.weights_exp_005.h5\n",
      "457/457 [==============================] - 8s 19ms/step - loss: 0.7473 - accuracy: 0.7005 - val_loss: 0.7493 - val_accuracy: 0.6772 - lr: 1.0000e-04\n",
      "Epoch 15/40\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.7320 - accuracy: 0.7040\n",
      "Epoch 15: val_loss improved from 0.74934 to 0.74888, saving model to /home/app/src/experiments/inceptionv3_GRU/exp_006/model.weights_exp_005.h5\n",
      "457/457 [==============================] - 9s 19ms/step - loss: 0.7320 - accuracy: 0.7040 - val_loss: 0.7489 - val_accuracy: 0.6781 - lr: 1.0000e-04\n",
      "Epoch 16/40\n",
      "454/457 [============================>.] - ETA: 0s - loss: 0.7226 - accuracy: 0.7095\n",
      "Epoch 16: val_loss did not improve from 0.74888\n",
      "457/457 [==============================] - 9s 19ms/step - loss: 0.7223 - accuracy: 0.7096 - val_loss: 0.7587 - val_accuracy: 0.6628 - lr: 1.0000e-04\n",
      "Epoch 17/40\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.7105 - accuracy: 0.7181\n",
      "Epoch 17: val_loss did not improve from 0.74888\n",
      "457/457 [==============================] - 9s 19ms/step - loss: 0.7105 - accuracy: 0.7181 - val_loss: 0.7518 - val_accuracy: 0.6789 - lr: 1.0000e-04\n",
      "Epoch 18/40\n",
      "454/457 [============================>.] - ETA: 0s - loss: 0.6999 - accuracy: 0.7227\n",
      "Epoch 18: val_loss did not improve from 0.74888\n",
      "457/457 [==============================] - 9s 19ms/step - loss: 0.7015 - accuracy: 0.7222 - val_loss: 0.7546 - val_accuracy: 0.6695 - lr: 1.0000e-04\n",
      "Epoch 19/40\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.6873 - accuracy: 0.7267\n",
      "Epoch 19: val_loss did not improve from 0.74888\n",
      "457/457 [==============================] - 8s 18ms/step - loss: 0.6873 - accuracy: 0.7267 - val_loss: 0.7549 - val_accuracy: 0.6695 - lr: 5.0000e-05\n",
      "Epoch 20/40\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.6807 - accuracy: 0.7354\n",
      "Epoch 20: val_loss did not improve from 0.74888\n",
      "457/457 [==============================] - 8s 18ms/step - loss: 0.6807 - accuracy: 0.7354 - val_loss: 0.7583 - val_accuracy: 0.6719 - lr: 5.0000e-05\n",
      "Epoch 21/40\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.6704 - accuracy: 0.7390\n",
      "Epoch 21: val_loss did not improve from 0.74888\n",
      "457/457 [==============================] - 10s 22ms/step - loss: 0.6704 - accuracy: 0.7390 - val_loss: 0.7606 - val_accuracy: 0.6778 - lr: 5.0000e-05\n",
      "Epoch 22/40\n",
      "456/457 [============================>.] - ETA: 0s - loss: 0.6659 - accuracy: 0.7439\n",
      "Epoch 22: val_loss did not improve from 0.74888\n",
      "457/457 [==============================] - 11s 23ms/step - loss: 0.6664 - accuracy: 0.7438 - val_loss: 0.7609 - val_accuracy: 0.6682 - lr: 2.5000e-05\n",
      "Epoch 23/40\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.6624 - accuracy: 0.7441\n",
      "Epoch 23: val_loss did not improve from 0.74888\n",
      "457/457 [==============================] - 9s 21ms/step - loss: 0.6624 - accuracy: 0.7441 - val_loss: 0.7629 - val_accuracy: 0.6687 - lr: 2.5000e-05\n",
      "Epoch 24/40\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.6584 - accuracy: 0.7427\n",
      "Epoch 24: val_loss did not improve from 0.74888\n",
      "457/457 [==============================] - 14s 32ms/step - loss: 0.6584 - accuracy: 0.7427 - val_loss: 0.7667 - val_accuracy: 0.6768 - lr: 2.5000e-05\n",
      "Epoch 25/40\n",
      "456/457 [============================>.] - ETA: 0s - loss: 0.6533 - accuracy: 0.7515\n",
      "Epoch 25: val_loss did not improve from 0.74888\n",
      "457/457 [==============================] - 23s 51ms/step - loss: 0.6532 - accuracy: 0.7516 - val_loss: 0.7659 - val_accuracy: 0.6689 - lr: 1.2500e-05\n",
      "Epoch 26/40\n",
      "456/457 [============================>.] - ETA: 0s - loss: 0.6514 - accuracy: 0.7519\n",
      "Epoch 26: val_loss did not improve from 0.74888\n",
      "457/457 [==============================] - 12s 27ms/step - loss: 0.6512 - accuracy: 0.7520 - val_loss: 0.7666 - val_accuracy: 0.6744 - lr: 1.2500e-05\n",
      "Epoch 27/40\n",
      "454/457 [============================>.] - ETA: 0s - loss: 0.6494 - accuracy: 0.7512\n",
      "Epoch 27: val_loss did not improve from 0.74888\n",
      "457/457 [==============================] - 9s 19ms/step - loss: 0.6496 - accuracy: 0.7511 - val_loss: 0.7675 - val_accuracy: 0.6690 - lr: 1.2500e-05\n",
      "Epoch 28/40\n",
      "455/457 [============================>.] - ETA: 0s - loss: 0.6474 - accuracy: 0.7546\n",
      "Epoch 28: val_loss did not improve from 0.74888\n",
      "457/457 [==============================] - 9s 20ms/step - loss: 0.6483 - accuracy: 0.7544 - val_loss: 0.7688 - val_accuracy: 0.6652 - lr: 6.2500e-06\n",
      "Epoch 29/40\n",
      "454/457 [============================>.] - ETA: 0s - loss: 0.6472 - accuracy: 0.7528\n",
      "Epoch 29: val_loss did not improve from 0.74888\n",
      "457/457 [==============================] - 9s 20ms/step - loss: 0.6471 - accuracy: 0.7524 - val_loss: 0.7687 - val_accuracy: 0.6657 - lr: 6.2500e-06\n",
      "Epoch 30/40\n",
      "454/457 [============================>.] - ETA: 0s - loss: 0.6403 - accuracy: 0.7556\n",
      "Epoch 30: val_loss did not improve from 0.74888\n",
      "457/457 [==============================] - 10s 21ms/step - loss: 0.6405 - accuracy: 0.7557 - val_loss: 0.7692 - val_accuracy: 0.6650 - lr: 6.2500e-06\n",
      "Epoch 31/40\n",
      "455/457 [============================>.] - ETA: 0s - loss: 0.6419 - accuracy: 0.7541\n",
      "Epoch 31: val_loss did not improve from 0.74888\n",
      "457/457 [==============================] - 9s 19ms/step - loss: 0.6416 - accuracy: 0.7544 - val_loss: 0.7697 - val_accuracy: 0.6653 - lr: 3.1250e-06\n",
      "Epoch 32/40\n",
      "456/457 [============================>.] - ETA: 0s - loss: 0.6416 - accuracy: 0.7557\n",
      "Epoch 32: val_loss did not improve from 0.74888\n",
      "457/457 [==============================] - 9s 19ms/step - loss: 0.6417 - accuracy: 0.7556 - val_loss: 0.7702 - val_accuracy: 0.6682 - lr: 3.1250e-06\n",
      "Epoch 33/40\n",
      "455/457 [============================>.] - ETA: 0s - loss: 0.6389 - accuracy: 0.7523\n",
      "Epoch 33: val_loss did not improve from 0.74888\n",
      "457/457 [==============================] - 10s 21ms/step - loss: 0.6388 - accuracy: 0.7522 - val_loss: 0.7702 - val_accuracy: 0.6652 - lr: 3.1250e-06\n",
      "Epoch 34/40\n",
      "455/457 [============================>.] - ETA: 0s - loss: 0.6385 - accuracy: 0.7584\n",
      "Epoch 34: val_loss did not improve from 0.74888\n",
      "457/457 [==============================] - 9s 19ms/step - loss: 0.6386 - accuracy: 0.7585 - val_loss: 0.7703 - val_accuracy: 0.6647 - lr: 1.5625e-06\n",
      "Epoch 35/40\n",
      "454/457 [============================>.] - ETA: 0s - loss: 0.6414 - accuracy: 0.7567\n",
      "Epoch 35: val_loss did not improve from 0.74888\n",
      "457/457 [==============================] - 11s 25ms/step - loss: 0.6412 - accuracy: 0.7566 - val_loss: 0.7706 - val_accuracy: 0.6652 - lr: 1.5625e-06\n",
      "Epoch 36/40\n",
      "455/457 [============================>.] - ETA: 0s - loss: 0.6401 - accuracy: 0.7568\n",
      "Epoch 36: val_loss did not improve from 0.74888\n",
      "457/457 [==============================] - 10s 22ms/step - loss: 0.6405 - accuracy: 0.7567 - val_loss: 0.7711 - val_accuracy: 0.6677 - lr: 1.5625e-06\n",
      "Epoch 37/40\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.6410 - accuracy: 0.7588\n",
      "Epoch 37: val_loss did not improve from 0.74888\n",
      "457/457 [==============================] - 9s 20ms/step - loss: 0.6410 - accuracy: 0.7588 - val_loss: 0.7709 - val_accuracy: 0.6649 - lr: 1.0000e-06\n",
      "Epoch 38/40\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.6386 - accuracy: 0.7603\n",
      "Epoch 38: val_loss did not improve from 0.74888\n",
      "457/457 [==============================] - 9s 20ms/step - loss: 0.6386 - accuracy: 0.7603 - val_loss: 0.7711 - val_accuracy: 0.6649 - lr: 1.0000e-06\n",
      "Epoch 39/40\n",
      "455/457 [============================>.] - ETA: 0s - loss: 0.6395 - accuracy: 0.7593\n",
      "Epoch 39: val_loss did not improve from 0.74888\n",
      "457/457 [==============================] - 9s 19ms/step - loss: 0.6397 - accuracy: 0.7592 - val_loss: 0.7711 - val_accuracy: 0.6650 - lr: 1.0000e-06\n",
      "Epoch 40/40\n",
      "454/457 [============================>.] - ETA: 0s - loss: 0.6399 - accuracy: 0.7583\n",
      "Epoch 40: val_loss did not improve from 0.74888\n",
      "457/457 [==============================] - 9s 19ms/step - loss: 0.6401 - accuracy: 0.7583 - val_loss: 0.7712 - val_accuracy: 0.6655 - lr: 1.0000e-06\n",
      "256/256 [==============================] - 2s 9ms/step - loss: 0.7448 - accuracy: 0.6841\n",
      "Test accuracy: 68.41%\n"
     ]
    }
   ],
   "source": [
    "def get_sequence_model():\n",
    "    class_vocab = label_processor.get_vocabulary()\n",
    "\n",
    "    frame_features_input = keras.Input((MAX_SEQ_LENGTH, NUM_FEATURES))\n",
    "    mask_input = keras.Input((MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "\n",
    "    # Refer to the following tutorial to understand the significance of using `mask`:\n",
    "    # https://keras.io/api/layers/recurrent_layers/gru/\n",
    "    x = keras.layers.GRU(16, return_sequences=True)(\n",
    "        frame_features_input, mask=mask_input\n",
    "    )\n",
    "    x = keras.layers.GRU(8)(x)\n",
    "    x = keras.layers.Dropout(0.4)(x)\n",
    "    x = keras.layers.Dense(8, activation=\"relu\")(x)\n",
    "    output = keras.layers.Dense(len(class_vocab), activation=\"softmax\")(x)\n",
    "\n",
    "    rnn_model = keras.Model([frame_features_input, mask_input], output)\n",
    "\n",
    "    rnn_model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\", optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return rnn_model\n",
    "\n",
    "\n",
    "# Utility for running experiments.\n",
    "def run_experiment():\n",
    "    filepath = \"/home/app/src/experiments/inceptionv3_GRU/exp_006/model.weights_exp_005.h5\"\n",
    "    filepath_log = '/home/app/src/experiments/inceptionv3_GRU/logs'\n",
    "\n",
    "    reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n",
    "                              patience=3, min_lr=0.000001)\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "        filepath, save_weights_only=True, save_best_only=True, verbose=1)\n",
    "    tensor_board = keras.callbacks.TensorBoard (filepath_log)\n",
    "\n",
    "    seq_model = get_sequence_model()\n",
    "    history = seq_model.fit(\n",
    "        [train_data[0], train_data[1]],\n",
    "        train_labels,\n",
    "        validation_split=0.3,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=[checkpoint, reduce_lr, tensor_board],\n",
    "    )\n",
    "\n",
    "    seq_model.load_weights(filepath)\n",
    "    _, accuracy = seq_model.evaluate([test_data[0], test_data[1]], test_labels)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "\n",
    "    return history, seq_model\n",
    "\n",
    "\n",
    "_, sequence_model = run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": [
    "%tensorboard == logdir /home/app/src/experiments/inceptionv3_GRU/logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test video path: /workspaces/final-project-shot-type/data/shot-type-dataset/trailer_v3/test/shot_tt2517260_0047.mp4\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sequence_prediction' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [19], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m test_video \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mchoice(test_df[\u001b[39m\"\u001b[39m\u001b[39mvideo_name\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mvalues\u001b[39m.\u001b[39mtolist())\n\u001b[1;32m     38\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTest video path: \u001b[39m\u001b[39m{\u001b[39;00mtest_video\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 39\u001b[0m test_frames \u001b[39m=\u001b[39m sequence_prediction(test_video)\n\u001b[1;32m     40\u001b[0m \u001b[39m# to_gif(test_frames[:MAX_SEQ_LENGTH])\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sequence_prediction' is not defined"
     ]
    }
   ],
   "source": [
    "# def prepare_single_video(frames):\n",
    "#     frames = frames[None, ...]\n",
    "#     frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "#     frame_features = np.zeros(shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\")\n",
    "\n",
    "#     for i, batch in enumerate(frames):\n",
    "#         video_length = batch.shape[0]\n",
    "#         length = min(MAX_SEQ_LENGTH, video_length)\n",
    "#         for j in range(length):\n",
    "#             frame_features[i, j, :] = feature_extractor.predict(batch[None, j, :])\n",
    "#         frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n",
    "\n",
    "#     return frame_features, frame_mask\n",
    "\n",
    "\n",
    "# def sequence_prediction(path):\n",
    "#     class_vocab = label_processor.get_vocabulary()\n",
    "\n",
    "#     frames = load_video(os.path.join(\"test\", path))\n",
    "#     frame_features, frame_mask = prepare_single_video(frames)\n",
    "#     probabilities = sequence_model.predict([frame_features, frame_mask])[0]\n",
    "\n",
    "#     for i in np.argsort(probabilities)[::-1]:\n",
    "#         print(f\"  {class_vocab[i]}: {probabilities[i] * 100:5.2f}%\")\n",
    "#     return frames\n",
    "\n",
    "\n",
    "# This utility is for visualization.\n",
    "# Referenced from:\n",
    "# https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub\n",
    "# def to_gif(images):\n",
    "#     converted_images = images.astype(np.uint8)\n",
    "#     imageio.mimsave(\"animation.gif\", converted_images, fps=10)\n",
    "#     return embed.embed_file(\"animation.gif\")\n",
    "\n",
    "\n",
    "test_video = np.random.choice(test_df[\"video_name\"].values.tolist())\n",
    "print(f\"Test video path: {test_video}\")\n",
    "test_frames = sequence_prediction(test_video)\n",
    "# to_gif(test_frames[:MAX_SEQ_LENGTH])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
